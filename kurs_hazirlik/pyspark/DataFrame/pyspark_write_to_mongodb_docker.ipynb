{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(\"/opt/manual/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run -d --name some-mongo \\\n",
    "-p 27017:27017 \\\n",
    "-e MONGO_INITDB_ROOT_USERNAME=mongoadmin \\\n",
    "-e MONGO_INITDB_ROOT_PASSWORD=secret \\\n",
    "-v mongodb:/data/db \\\n",
    "mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Connect mongo shell and create user\n",
    "    mongo -u mongoadmin -p secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     use test\n",
    "     db.createUser(\n",
    "      {\n",
    "        user: \"train\",\n",
    "        pwd:  passwordPrompt(),   // or cleartext password\n",
    "        roles: [ { role: \"readWrite\", db: \"test\" },\n",
    "                 { role: \"read\", db: \"reporting\" } ]\n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"MongoDB Example\") \\\n",
    ".master(\"local[2]\") \\\n",
    ".config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\") \\\n",
    ".config(\"spark.mongodb.auth.uri\",\"mongodb://train:<password_here>@127.0.0.1:27017/test\") \\\n",
    ".config(\"spark.mongodb.input.uri\",\n",
    "        \"mongodb://127.0.0.1:27017/test.people\") \\\n",
    ".config(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1:27017/test.people\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".option(\"header\", True) \\\n",
    ".csv(\"file:///home/train/datasets/simple_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\n",
    "\"test\").option(\"collection\", \"people\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
